{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "83534b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "* fit k: performance of var_direct_np\n",
      "Fitted model: Performance = 0.0000006664 * k**0.769\n",
      "R² score: 0.9239; Cross-validation R²: 0.9238 ± 0.0015\n",
      "* fit o: performance of var_hypo\n",
      "Fitted model: Performance = 0.0003939771 * o**1.954\n",
      "R² score: 0.8363; Cross-validation R²: 0.8347 ± 0.0264\n",
      "\n",
      "********************************************************************************\n",
      "* fit k, o: performance of var_direct_np\n",
      "Fitted model: Performance = 0.0000006062 * k**0.769 * o**0.099\n",
      "R² score: 0.9241; Cross-validation R²: 0.9240 ± 0.0015\n",
      "* fit k, o: performance of var_hypo\n",
      "Fitted model: Performance = 0.0007118038 * k**-0.064 * o**1.954\n",
      "R² score: 0.9005; Cross-validation R²: 0.8967 ± 0.0066\n",
      "\n",
      "********************************************************************************\n",
      "* fit n, k, o, g: performance of var_direct_np\n",
      "Fitted model: Performance = 0.0000005710 * n**0.035 * k**0.705 * o**0.099 * g**0.012\n",
      "R² score: 0.9246; Cross-validation R²: 0.9245 ± 0.0014\n",
      "* fit n, k, o, g: performance of var_hypo\n",
      "Fitted model: Performance = 0.0006619501 * n**-0.034 * k**-0.001 * o**1.954 * g**0.126\n",
      "R² score: 0.9091; Cross-validation R²: 0.9071 ± 0.0077\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import r2_score\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Iterable, Sequence, Tuple, Optional, Union\n",
    "import warnings\n",
    "\n",
    "# Suppress sklearn numerical warnings\n",
    "warnings.filterwarnings('ignore', message='Ill-conditioned matrix')\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='sklearn')\n",
    "\n",
    "@dataclass\n",
    "class PowerLawModel:\n",
    "    \"\"\"Power law regression model for algorithm performance prediction.\"\"\"\n",
    "    coefficients: np.ndarray\n",
    "    intercept: float\n",
    "    r2_score: float\n",
    "    parameter_names: list\n",
    "    \n",
    "    def predict(self, **kwargs) -> Union[np.ndarray, float]:\n",
    "        \"\"\"Predict performance using parameter values.\"\"\"\n",
    "        # Ensure all required parameters are provided\n",
    "        for param in self.parameter_names:\n",
    "            if param not in kwargs:\n",
    "                raise ValueError(f\"Missing required parameter: {param}\")\n",
    "        \n",
    "        # Create feature matrix\n",
    "        features = np.column_stack([np.log(np.maximum(kwargs[param], 1e-8)) \n",
    "                                   for param in self.parameter_names])\n",
    "        \n",
    "        # Handle single prediction vs batch prediction\n",
    "        if features.shape[0] == 1:\n",
    "            log_prediction = self.intercept + np.dot(features[0], self.coefficients)\n",
    "            return np.exp(log_prediction)\n",
    "        else:\n",
    "            log_predictions = self.intercept + np.dot(features, self.coefficients)\n",
    "            return np.exp(log_predictions)\n",
    "    \n",
    "    def get_formula(self) -> str:\n",
    "        \"\"\"Return human-readable power law formula.\"\"\"\n",
    "        a = np.exp(self.intercept)\n",
    "        terms = [f\"{param}**{coef:.3f}\" for param, coef in zip(self.parameter_names, self.coefficients)]\n",
    "        return f\"Performance = {a:.10f} * \" + \" * \".join(terms)\n",
    "\n",
    "class PowerLawRegressor:\n",
    "    \"\"\"Robust power law regression with flexible parameter handling.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.parameter_names = None\n",
    "    \n",
    "    def fit(self, data: Dict[str, np.ndarray], performance: np.ndarray, \n",
    "            parameters: Optional[list] = None) -> PowerLawModel:\n",
    "        \"\"\"\n",
    "        Fit power law model to performance data.\n",
    "        \n",
    "        Args:\n",
    "            data: Dictionary with parameter names as keys and arrays as values\n",
    "            performance: Array of performance measurements\n",
    "            parameters: List of parameter names to include in model (None = all)\n",
    "        \"\"\"\n",
    "        if parameters is None:\n",
    "            parameters = list(data.keys())\n",
    "        \n",
    "        self.parameter_names = parameters\n",
    "        \n",
    "        # Create log-transformed feature matrix\n",
    "        epsilon = 1e-8\n",
    "        features = np.column_stack([\n",
    "            np.log(np.maximum(data[param], epsilon)) for param in parameters\n",
    "        ])\n",
    "        log_performance = np.log(np.maximum(performance, epsilon))\n",
    "        \n",
    "        # Fit ridge regression for numerical stability\n",
    "        ridge = Ridge(alpha=1e-6)\n",
    "        ridge.fit(features, log_performance)\n",
    "        \n",
    "        # Calculate R² score\n",
    "        log_predictions = ridge.predict(features)\n",
    "        r2 = r2_score(log_performance, log_predictions)\n",
    "        \n",
    "        self.model = PowerLawModel(\n",
    "            coefficients=ridge.coef_,\n",
    "            intercept=ridge.intercept_,\n",
    "            r2_score=r2,\n",
    "            parameter_names=parameters\n",
    "        )\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    def cross_validate(self, data: Dict[str, np.ndarray], performance: np.ndarray, \n",
    "                      parameters: Optional[list] = None, cv: int = 5) -> Dict:\n",
    "        \"\"\"Perform cross-validation for power law model.\"\"\"\n",
    "        if parameters is None:\n",
    "            parameters = list(data.keys())\n",
    "        \n",
    "        epsilon = 1e-8\n",
    "        features = np.column_stack([\n",
    "            np.log(np.maximum(data[param], epsilon)) for param in parameters\n",
    "        ])\n",
    "        log_performance = np.log(np.maximum(performance, epsilon))\n",
    "        \n",
    "        ridge = Ridge(alpha=1e-6)\n",
    "        scores = cross_val_score(ridge, features, log_performance, cv=cv, scoring='r2')\n",
    "        \n",
    "        return {\n",
    "            'mean_r2': scores.mean(),\n",
    "            'std_r2': scores.std(),\n",
    "            'individual_scores': scores\n",
    "        }\n",
    "\n",
    "def generate_realistic_test_data(\n",
    "        n_samples: int = 1000, \n",
    "        o_fixed: Optional[int] = None, \n",
    "        g_fixed: Optional[int] = None, \n",
    "        random_seed: int = 42,\n",
    "    ) -> Dict[str, npt.NDArray[np.float64]]:\n",
    "    \"\"\"\n",
    "    Generate synthetic performance data with consistent power law relationship.\n",
    "    \n",
    "    Args:\n",
    "        n_samples: Number of base samples for n and k\n",
    "        o_fixed: If specified, use this fixed value for o\n",
    "        g_fixed: If specified, use this fixed value for g  \n",
    "        random_seed: Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing parameter arrays and performance measurements\n",
    "    \"\"\"\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    # Generate base parameter samples\n",
    "    n_base = np.random.randint(10, 1000, n_samples)\n",
    "    k_base = np.random.randint(1, 100, n_samples)\n",
    "    \n",
    "    # Determine o and g values\n",
    "    if o_fixed is not None and g_fixed is not None:\n",
    "        # Fixed o and g case\n",
    "        o_values = [o_fixed]\n",
    "        g_values = [g_fixed]\n",
    "    elif o_fixed is not None:\n",
    "        # Fixed o, vary g\n",
    "        o_values = [o_fixed]\n",
    "        g_values = list(range(1, 6))\n",
    "    elif g_fixed is not None:\n",
    "        # Fixed g, vary o\n",
    "        o_values = list(range(1, 6))\n",
    "        g_values = [g_fixed]\n",
    "    else:\n",
    "        # Full grid of o and g values\n",
    "        o_values = list(range(1, 6))\n",
    "        g_values = list(range(1, 6))\n",
    "    \n",
    "    # Generate complete dataset\n",
    "    all_n, all_k, all_o, all_g, all_performance = [], [], [], [], []\n",
    "    \n",
    "    # True power law parameters - consistent across all data\n",
    "    true_a = 0.001\n",
    "    true_n_exp = 1.2\n",
    "    true_k_exp = 0.8\n",
    "    true_o_exp = 0.6\n",
    "    true_g_exp = 0.4\n",
    "    \n",
    "    for o_val in o_values:\n",
    "        for g_val in g_values:\n",
    "            # Use same n,k samples for each (o,g) combination\n",
    "            n_samples_og = n_base.copy()\n",
    "            k_samples_og = k_base.copy()\n",
    "            o_samples_og = np.full(n_samples, o_val)\n",
    "            g_samples_og = np.full(n_samples, g_val)\n",
    "            \n",
    "            # Calculate true performance using consistent power law\n",
    "            true_performance = (true_a * \n",
    "                              (n_samples_og ** true_n_exp) * \n",
    "                              (k_samples_og ** true_k_exp) * \n",
    "                              (o_samples_og ** true_o_exp) * \n",
    "                              (g_samples_og ** true_g_exp))\n",
    "            \n",
    "            # Add realistic multiplicative noise\n",
    "            noise = np.random.lognormal(0, 0.1, n_samples)\n",
    "            noisy_performance = true_performance * noise\n",
    "            \n",
    "            all_n.extend(n_samples_og)\n",
    "            all_k.extend(k_samples_og)\n",
    "            all_o.extend(o_samples_og)\n",
    "            all_g.extend(g_samples_og)\n",
    "            all_performance.extend(noisy_performance)\n",
    "    \n",
    "    result = {\n",
    "        'n': np.array(all_n),\n",
    "        'k': np.array(all_k),\n",
    "        'o': np.array(all_o),\n",
    "        'g': np.array(all_g),\n",
    "        'performance': np.array(all_performance)\n",
    "    }\n",
    "    \n",
    "    # Print generation summary\n",
    "    print(f\"Generated {len(all_n)} total data points\")\n",
    "    print(f\"True relationship: Performance = {true_a} × n^{true_n_exp} × k^{true_k_exp} × o^{true_o_exp} × g^{true_g_exp}\")\n",
    "    print(f\"Parameter ranges: n=[{result['n'].min()}-{result['n'].max()}], k=[{result['k'].min()}-{result['k'].max()}], o=[{result['o'].min()}-{result['o'].max()}], g=[{result['g'].min()}-{result['g'].max()}]\")\n",
    "    print(f\"Performance range: [{result['performance'].min():.2e} - {result['performance'].max():.2e}]\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "def analyze_power_law_regression(data: Dict[str, npt.NDArray[np.float64]], \n",
    "                               scenarios: list) -> Dict:\n",
    "    \"\"\"\n",
    "    Comprehensive power law regression analysis for different parameter scenarios.\n",
    "    \n",
    "    Args:\n",
    "        data: Dictionary containing parameter arrays and performance\n",
    "        scenarios: List of parameter combinations to analyze\n",
    "    \"\"\"\n",
    "    \n",
    "    regressor = PowerLawRegressor()\n",
    "    results = {}\n",
    "    \n",
    "    performance = data['performance']\n",
    "    \n",
    "    for scenario in scenarios:\n",
    "        # print(f\"\\n=== Analyzing scenario: {scenario} ===\")\n",
    "        \n",
    "        # Fit model\n",
    "        model = regressor.fit(data, performance, parameters=scenario)\n",
    "        print(f\"Fitted model: {model.get_formula()}\")\n",
    "        print(f\"R² score: {model.r2_score:.4f}\", end = '; ')\n",
    "        \n",
    "        # Cross-validation\n",
    "        cv_results = regressor.cross_validate(data, performance, parameters=scenario)\n",
    "        print(f\"Cross-validation R²: {cv_results['mean_r2']:.4f} ± {cv_results['std_r2']:.4f}\")\n",
    "        \n",
    "        results[tuple(scenario)] = {\n",
    "            'model': model,\n",
    "            'cv_results': cv_results\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage scenarios\n",
    "def demonstrate_analysis():\n",
    "    \"\"\"Demonstrate different analysis scenarios.\"\"\"\n",
    "    \n",
    "    print(\"=== Scenario 1: Full 4-parameter analysis ===\")\n",
    "    data_full = generate_realistic_test_data(n_samples=500)\n",
    "    results_full = analyze_power_law_regression(data_full, [['n', 'k', 'o', 'g']])\n",
    "    return results_full #, results_fixed, results_subset\n",
    "\n",
    "def read_data(fn: str, function_name: str, parameter_names: dict[str, int]) -> dict[str, npt.NDArray[np.float64]]:\n",
    "    import json\n",
    "    with open(fn, 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "    assert function_name in json_data.keys()\n",
    "    samples = json_data[function_name]\n",
    "    # sample1 = samples[0]\n",
    "    # assert len(sample1) == len(parameter_names) + 1\n",
    "    samples_arr = np.array(samples, dtype=np.float64)\n",
    "    \n",
    "    data = {}\n",
    "    for parameter_name, i in parameter_names.items():\n",
    "        data[parameter_name] = samples_arr[:,i]\n",
    "    data['performance'] = samples_arr[:,-1]\n",
    "    \n",
    "    from pprint import pprint\n",
    "    # print('data:')\n",
    "    # pprint(data)\n",
    "    return data\n",
    "\n",
    "\n",
    "alg_names = ['var_direct_np', 'var_hypo']\n",
    "\n",
    "\n",
    "print('*'*80)\n",
    "print(f'* fit k: performance of var_direct_np')\n",
    "data = read_data('running_time_data_compute_var.json', 'var_direct_np', {'k': 1})\n",
    "results_full = analyze_power_law_regression(data, [['k']])\n",
    "\n",
    "print(f'* fit o: performance of var_hypo')\n",
    "data = read_data('running_time_data_compute_var.json', 'var_hypo', {'o': 2})\n",
    "results_full = analyze_power_law_regression(data, [['o']])\n",
    "\n",
    "print('\\n' + '*'*80)\n",
    "for alg_name in alg_names:\n",
    "    print(f'* fit k, o: performance of {alg_name}')\n",
    "    data = read_data('running_time_data_compute_var.json', alg_name, {'k': 1, 'o': 2})\n",
    "    results_full = analyze_power_law_regression(data, [['k', 'o']])\n",
    "\n",
    "print('\\n' + '*'*80)\n",
    "for alg_name in alg_names:\n",
    "    print(f'* fit n, k, o, g: performance of {alg_name}')\n",
    "    data = read_data('running_time_data_compute_var.json', alg_name, {'n': 0, 'k': 1, 'o': 2, 'g': 3})\n",
    "    results_full = analyze_power_law_regression(data, [['n', 'k', 'o', 'g']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "58f0b45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "* fit k: performance of mean_direct_np\n",
      "Fitted model: Performance = 0.0000001035 * k**0.951 + 0.0\n",
      "R² score: 0.9826; \n",
      "Fitting details: Standard power law R²: 0.9151; Additive constant R²: 0.9826 (improvement: +0.0675, method: Differential evolution with standard_start)\n",
      "Cross-validation R²: 0.9825 ± 0.0043\n",
      "\n",
      "********************************************************************************\n",
      "* fit o: performance of mean_hypo\n",
      "Fitted model: Performance = 0.0002021981 * o**0.855\n",
      "R² score: 0.8272; \n",
      "Fitting details: Standard power law R²: 0.8272; Additive constant R² (0.7051) worse than standard, using standard model\n",
      "Cross-validation R²: 0.6933 ± 0.0181\n",
      "\n",
      "********************************************************************************\n",
      "* fit k, o: performance of mean_direct_np\n",
      "Fitted model: Performance = 0.0000000927 * k**0.951 * o**0.103 + 0.0\n",
      "R² score: 0.9861; \n",
      "Fitting details: Standard power law R²: 0.9155; Additive constant R²: 0.9861 (improvement: +0.0706, method: Differential evolution with standard_start)\n",
      "Cross-validation R²: 0.9860 ± 0.0042\n",
      "\n",
      "********************************************************************************\n",
      "* fit k, o: performance of mean_hypo\n",
      "Fitted model: Performance = 0.0002383661 * k**-0.018 * o**0.858\n",
      "R² score: 0.8544; \n",
      "Fitting details: Standard power law R²: 0.8544; Additive constant R² (0.7501) worse than standard, using standard model\n",
      "Cross-validation R²: 0.7411 ± 0.0424\n",
      "\n",
      "********************************************************************************\n",
      "* fit n, k, o: performance of mean_hypo\n",
      "Fitted model: Performance = 0.0002412678 * n**-0.009 * k**-0.001 * o**0.858\n",
      "R² score: 0.8563; \n",
      "Fitting details: Standard power law R²: 0.8563; Additive constant R² (0.7502) worse than standard, using standard model\n",
      "Cross-validation R²: 0.7429 ± 0.0446\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.optimize import minimize, differential_evolution\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Iterable, Sequence, Tuple, Optional, Union\n",
    "import warnings\n",
    "\n",
    "# Suppress sklearn numerical warnings\n",
    "warnings.filterwarnings('ignore', message='Ill-conditioned matrix')\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='sklearn')\n",
    "\n",
    "@dataclass\n",
    "class PowerLawModel:\n",
    "    \"\"\"Power law regression model for algorithm performance prediction.\"\"\"\n",
    "    coefficients: np.ndarray\n",
    "    intercept: float\n",
    "    additive_constant: float\n",
    "    r2_score: float\n",
    "    parameter_names: list\n",
    "    has_additive_constant: bool = False\n",
    "    fitting_summary: str = \"\"\n",
    "    \n",
    "    def predict(self, **kwargs) -> Union[np.ndarray, float]:\n",
    "        \"\"\"Predict performance using parameter values.\"\"\"\n",
    "        # Ensure all required parameters are provided\n",
    "        for param in self.parameter_names:\n",
    "            if param not in kwargs:\n",
    "                raise ValueError(f\"Missing required parameter: {param}\")\n",
    "        \n",
    "        if self.has_additive_constant:\n",
    "            # Direct calculation for additive constant model\n",
    "            # Performance = a × n^b1 × k^b2 × ... + c\n",
    "            a = self.coefficients[0]  # First coefficient is 'a'\n",
    "            exponents = self.coefficients[1:]  # Rest are exponents\n",
    "            \n",
    "            # Handle single vs batch prediction\n",
    "            param_values = [kwargs[param] for param in self.parameter_names]\n",
    "            param_arrays = [np.atleast_1d(val) for val in param_values]\n",
    "            \n",
    "            # Calculate power law part\n",
    "            power_law_part = a * np.ones(len(param_arrays[0]))\n",
    "            for i, param_array in enumerate(param_arrays):\n",
    "                power_law_part *= param_array ** exponents[i]\n",
    "            \n",
    "            result = power_law_part + self.additive_constant\n",
    "            \n",
    "            # Return scalar if input was scalar\n",
    "            if all(len(arr) == 1 for arr in param_arrays):\n",
    "                return float(result[0])\n",
    "            return result\n",
    "            \n",
    "        else:\n",
    "            # Original log-space calculation\n",
    "            features = np.column_stack([np.log(np.maximum(kwargs[param], 1e-8)) \n",
    "                                       for param in self.parameter_names])\n",
    "            \n",
    "            # Handle single prediction vs batch prediction\n",
    "            if features.shape[0] == 1:\n",
    "                log_prediction = self.intercept + np.dot(features[0], self.coefficients)\n",
    "                return np.exp(log_prediction)\n",
    "            else:\n",
    "                log_predictions = self.intercept + np.dot(features, self.coefficients)\n",
    "                return np.exp(log_predictions)\n",
    "    \n",
    "    def get_formula(self) -> str:\n",
    "        \"\"\"Return human-readable power law formula.\"\"\"\n",
    "        if self.has_additive_constant:\n",
    "            a = self.coefficients[0]\n",
    "            exponents = self.coefficients[1:]\n",
    "            terms = [f\"{param}**{exp:.3f}\" for param, exp in zip(self.parameter_names, exponents)]\n",
    "            power_part = f\"{a:.10f} * \" + \" * \".join(terms)\n",
    "            return f\"Performance = {power_part} + {self.additive_constant}\"\n",
    "        else:\n",
    "            a = np.exp(self.intercept)\n",
    "            terms = [f\"{param}**{coef:.3f}\" for param, coef in zip(self.parameter_names, self.coefficients)]\n",
    "            return f\"Performance = {a:.10f} * \" + \" * \".join(terms)\n",
    "\n",
    "class PowerLawRegressor:\n",
    "    \"\"\"Robust power law regression with flexible parameter handling and additive constant option.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.parameter_names = None\n",
    "    \n",
    "    def fit(self, data: Dict[str, np.ndarray], performance: np.ndarray, \n",
    "            parameters: Optional[list] = None, \n",
    "            include_additive_constant: bool = False) -> PowerLawModel:\n",
    "        \"\"\"\n",
    "        Fit power law model to performance data.\n",
    "        \n",
    "        Args:\n",
    "            data: Dictionary with parameter names as keys and arrays as values\n",
    "            performance: Array of performance measurements\n",
    "            parameters: List of parameter names to include in model (None = all)\n",
    "            include_additive_constant: If True, fit model with additive constant\n",
    "        \"\"\"\n",
    "        if parameters is None:\n",
    "            parameters = list(data.keys())\n",
    "        \n",
    "        self.parameter_names = parameters\n",
    "        \n",
    "        if include_additive_constant:\n",
    "            return self._fit_with_additive_constant(data, performance, parameters)\n",
    "        else:\n",
    "            return self._fit_standard(data, performance, parameters)\n",
    "    \n",
    "    def _fit_standard(self, data: Dict[str, np.ndarray], performance: np.ndarray,\n",
    "                     parameters: list) -> PowerLawModel:\n",
    "        \"\"\"Fit standard power law model without additive constant.\"\"\"\n",
    "        # Create log-transformed feature matrix\n",
    "        epsilon = 1e-8\n",
    "        features = np.column_stack([\n",
    "            np.log(np.maximum(data[param], epsilon)) for param in parameters\n",
    "        ])\n",
    "        log_performance = np.log(np.maximum(performance, epsilon))\n",
    "        \n",
    "        # Fit ridge regression for numerical stability\n",
    "        ridge = Ridge(alpha=1e-6)\n",
    "        ridge.fit(features, log_performance)\n",
    "        \n",
    "        # Calculate R² score\n",
    "        log_predictions = ridge.predict(features)\n",
    "        r2 = r2_score(log_performance, log_predictions)\n",
    "        \n",
    "        return PowerLawModel(\n",
    "            coefficients=ridge.coef_,\n",
    "            intercept=ridge.intercept_,\n",
    "            additive_constant=0.0,\n",
    "            r2_score=r2,\n",
    "            parameter_names=parameters,\n",
    "            has_additive_constant=False,\n",
    "            fitting_summary=\"\"\n",
    "        )\n",
    "    \n",
    "    def _fit_with_additive_constant(self, data: Dict[str, np.ndarray], performance: np.ndarray,\n",
    "                                   parameters: list) -> PowerLawModel:\n",
    "        \"\"\"Fit power law model with additive constant using non-linear optimization.\"\"\"\n",
    "        \n",
    "        # First get the standard power law fit as baseline\n",
    "        standard_model = self._fit_standard(data, performance, parameters)\n",
    "        standard_predictions = standard_model.predict(**{param: data[param] for param in parameters})\n",
    "        standard_sse = np.sum((performance - standard_predictions) ** 2)\n",
    "        \n",
    "        summary_parts = []\n",
    "        summary_parts.append(f\"Standard power law R²: {standard_model.r2_score:.4f}\")\n",
    "        \n",
    "        def objective_function(params):\n",
    "            \"\"\"Objective function to minimize: sum of squared residuals.\"\"\"\n",
    "            try:\n",
    "                a = params[0]  # Multiplicative coefficient\n",
    "                exponents = params[1:-1]  # Exponents for each parameter\n",
    "                c = params[-1]  # Additive constant\n",
    "                \n",
    "                # Calculate predicted performance: a × n^b1 × k^b2 × ... + c\n",
    "                predicted = a * np.ones(len(performance))\n",
    "                for i, param in enumerate(parameters):\n",
    "                    predicted *= data[param] ** exponents[i]\n",
    "                predicted += c\n",
    "                \n",
    "                # Return sum of squared errors\n",
    "                residuals = performance - predicted\n",
    "                return np.sum(residuals ** 2)\n",
    "                \n",
    "            except (OverflowError, ValueError, FloatingPointError):\n",
    "                return 1e15  # Return large value for invalid parameters\n",
    "        \n",
    "        # Strategy 1: Start from standard power law solution (c=0)\n",
    "        a_init = np.exp(standard_model.intercept)\n",
    "        exponents_init = standard_model.coefficients.copy()\n",
    "        initial_guess_1 = [a_init] + list(exponents_init) + [0.0]\n",
    "        \n",
    "        # Strategy 2: Try small positive additive constant\n",
    "        mean_performance = np.mean(performance)\n",
    "        initial_guess_2 = [a_init] + list(exponents_init) + [0.01 * mean_performance]\n",
    "        \n",
    "        # Strategy 3: Try negative additive constant\n",
    "        initial_guess_3 = [a_init] + list(exponents_init) + [-0.01 * mean_performance]\n",
    "        \n",
    "        # Set reasonable bounds\n",
    "        bounds = []\n",
    "        bounds.append((1e-15, 1e15))  # Bounds for coefficient 'a'\n",
    "        for _ in parameters:\n",
    "            bounds.append((-10.0, 10.0))  # Bounds for exponents\n",
    "        bounds.append((-2*np.max(performance), 2*np.max(performance)))  # Bounds for additive constant\n",
    "        \n",
    "        # Try multiple optimization approaches with different starting points\n",
    "        best_result = None\n",
    "        best_objective = standard_sse  # Start with standard model performance\n",
    "        best_method = \"none\"\n",
    "        \n",
    "        optimization_attempts = [\n",
    "            (initial_guess_1, \"standard_start\"),\n",
    "            (initial_guess_2, \"positive_constant_start\"),\n",
    "            (initial_guess_3, \"negative_constant_start\")\n",
    "        ]\n",
    "        \n",
    "        for initial_guess, description in optimization_attempts:\n",
    "            # L-BFGS-B optimization\n",
    "            try:\n",
    "                result = minimize(objective_function, initial_guess, method='L-BFGS-B', \n",
    "                                bounds=bounds, options={'maxiter': 1000})\n",
    "                if result.success and result.fun < best_objective:\n",
    "                    best_result = result\n",
    "                    best_objective = result.fun\n",
    "                    best_method = f\"L-BFGS-B with {description}\"\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Try differential evolution from this starting point\n",
    "            try:\n",
    "                # Use tighter bounds around the initial guess for more focused search\n",
    "                tight_bounds = []\n",
    "                for i, val in enumerate(initial_guess):\n",
    "                    if i < len(bounds):\n",
    "                        lower = max(bounds[i][0], val * 0.1)\n",
    "                        upper = min(bounds[i][1], val * 10.0)\n",
    "                        tight_bounds.append((lower, upper))\n",
    "                    else:\n",
    "                        tight_bounds.append(bounds[i])\n",
    "                \n",
    "                result = differential_evolution(objective_function, tight_bounds, \n",
    "                                              seed=42, maxiter=200)\n",
    "                if result.success and result.fun < best_objective:\n",
    "                    best_result = result\n",
    "                    best_objective = result.fun\n",
    "                    best_method = f\"Differential evolution with {description}\"\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # If no improvement found, return standard model\n",
    "        if best_result is None or best_objective >= standard_sse * 0.999:  # Allow tiny numerical differences\n",
    "            summary_parts.append(\"No improvement found with additive constant\")\n",
    "            standard_model.fitting_summary = \"; \".join(summary_parts)\n",
    "            return standard_model\n",
    "        \n",
    "        # Extract fitted parameters\n",
    "        a_fitted = best_result.x[0]\n",
    "        exponents_fitted = best_result.x[1:-1]\n",
    "        c_fitted = best_result.x[-1]\n",
    "        \n",
    "        # Calculate R² score for the fitted model\n",
    "        predicted_performance = a_fitted * np.ones(len(performance))\n",
    "        for i, param in enumerate(parameters):\n",
    "            predicted_performance *= data[param] ** exponents_fitted[i]\n",
    "        predicted_performance += c_fitted\n",
    "        \n",
    "        r2 = r2_score(performance, predicted_performance)\n",
    "        \n",
    "        # Verify this actually improved over standard model\n",
    "        if r2 < standard_model.r2_score * 0.999:  # Allow tiny numerical differences\n",
    "            summary_parts.append(f\"Additive constant R² ({r2:.4f}) worse than standard, using standard model\")\n",
    "            standard_model.fitting_summary = \"; \".join(summary_parts)\n",
    "            return standard_model\n",
    "        \n",
    "        improvement = r2 - standard_model.r2_score\n",
    "        summary_parts.append(f\"Additive constant R²: {r2:.4f} (improvement: +{improvement:.4f}, method: {best_method})\")\n",
    "        \n",
    "        # Package results\n",
    "        coefficients = np.array([a_fitted] + list(exponents_fitted))\n",
    "        \n",
    "        return PowerLawModel(\n",
    "            coefficients=coefficients,\n",
    "            intercept=0.0,  # Not used for additive constant model\n",
    "            additive_constant=c_fitted,\n",
    "            r2_score=r2,\n",
    "            parameter_names=parameters,\n",
    "            has_additive_constant=True,\n",
    "            fitting_summary=\"; \".join(summary_parts)\n",
    "        )\n",
    "    \n",
    "    def cross_validate(self, data: Dict[str, np.ndarray], performance: np.ndarray, \n",
    "                      parameters: Optional[list] = None, cv: int = 5,\n",
    "                      include_additive_constant: bool = False) -> Dict:\n",
    "        \"\"\"Perform cross-validation for power law model.\"\"\"\n",
    "        if parameters is None:\n",
    "            parameters = list(data.keys())\n",
    "        \n",
    "        if include_additive_constant:\n",
    "            # For additive constant models, we need custom CV since sklearn doesn't support our model\n",
    "            return self._cross_validate_additive_constant(data, performance, parameters, cv)\n",
    "        else:\n",
    "            # Standard CV for log-linear models\n",
    "            epsilon = 1e-8\n",
    "            features = np.column_stack([\n",
    "                np.log(np.maximum(data[param], epsilon)) for param in parameters\n",
    "            ])\n",
    "            log_performance = np.log(np.maximum(performance, epsilon))\n",
    "            \n",
    "            ridge = Ridge(alpha=1e-6)\n",
    "            scores = cross_val_score(ridge, features, log_performance, cv=cv, scoring='r2')\n",
    "            \n",
    "            return {\n",
    "                'mean_r2': scores.mean(),\n",
    "                'std_r2': scores.std(),\n",
    "                'individual_scores': scores\n",
    "            }\n",
    "    \n",
    "    def _cross_validate_additive_constant(self, data: Dict[str, np.ndarray], \n",
    "                                         performance: np.ndarray, parameters: list, \n",
    "                                         cv: int) -> Dict:\n",
    "        \"\"\"Custom cross-validation for additive constant models.\"\"\"\n",
    "        from sklearn.model_selection import KFold\n",
    "        \n",
    "        kf = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "        scores = []\n",
    "        \n",
    "        for train_idx, val_idx in kf.split(performance):\n",
    "            # Split data\n",
    "            train_data = {param: data[param][train_idx] for param in parameters}\n",
    "            train_performance = performance[train_idx]\n",
    "            val_data = {param: data[param][val_idx] for param in parameters}\n",
    "            val_performance = performance[val_idx]\n",
    "            \n",
    "            # Fit model on training data\n",
    "            try:\n",
    "                train_model = self._fit_with_additive_constant(train_data, train_performance, parameters)\n",
    "                \n",
    "                # Predict on validation data\n",
    "                val_predictions = train_model.predict(**val_data)\n",
    "                \n",
    "                # Calculate R² score\n",
    "                r2 = r2_score(val_performance, val_predictions)\n",
    "                scores.append(r2)\n",
    "                \n",
    "            except:\n",
    "                # If fitting fails, append a poor score\n",
    "                scores.append(-1.0)\n",
    "        \n",
    "        scores = np.array(scores)\n",
    "        return {\n",
    "            'mean_r2': scores.mean(),\n",
    "            'std_r2': scores.std(),\n",
    "            'individual_scores': scores\n",
    "        }\n",
    "\n",
    "def analyze_power_law_regression(data: Dict[str, npt.NDArray[np.float64]], \n",
    "                               scenarios: Optional[list] = None,\n",
    "                               include_additive_constant: bool = False,\n",
    "                               show_fitting_details: bool = False) -> Dict:\n",
    "    \"\"\"\n",
    "    Comprehensive power law regression analysis for different parameter scenarios.\n",
    "    \n",
    "    Args:\n",
    "        data: Dictionary containing parameter arrays and performance\n",
    "        scenarios: List of parameter combinations to analyze\n",
    "        include_additive_constant: If True, fit models with additive constants\n",
    "        show_fitting_details: If True, show detailed fitting information\n",
    "    \"\"\"\n",
    "    if scenarios is None:\n",
    "        scenarios = [\n",
    "            ['k', 'o'],\n",
    "        ]\n",
    "    \n",
    "    regressor = PowerLawRegressor()\n",
    "    results = {}\n",
    "    \n",
    "    performance = data['performance']\n",
    "    \n",
    "    for scenario in scenarios:\n",
    "        # Fit model\n",
    "        model = regressor.fit(data, performance, parameters=scenario, \n",
    "                            include_additive_constant=include_additive_constant)\n",
    "        print(f\"Fitted model: {model.get_formula()}\")\n",
    "        print(f\"R² score: {model.r2_score:.4f}\", end = '; ')\n",
    "        \n",
    "        # Show fitting summary if requested or if using additive constants\n",
    "        if (show_fitting_details or include_additive_constant) and model.fitting_summary:\n",
    "            print(f\"\\nFitting details: {model.fitting_summary}\")\n",
    "        \n",
    "        # Cross-validation\n",
    "        cv_results = regressor.cross_validate(data, performance, parameters=scenario,\n",
    "                                            include_additive_constant=include_additive_constant)\n",
    "        print(f\"Cross-validation R²: {cv_results['mean_r2']:.4f} ± {cv_results['std_r2']:.4f}\")\n",
    "        \n",
    "        results[tuple(scenario)] = {\n",
    "            'model': model,\n",
    "            'cv_results': cv_results\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage comparing both approaches\n",
    "def compare_models_with_and_without_constant(data, parameters):\n",
    "    \"\"\"Compare standard power law vs power law with additive constant.\"\"\"\n",
    "    \n",
    "    print(\"=== Standard Power Law Model ===\")\n",
    "    results_standard = analyze_power_law_regression(data, [parameters], include_additive_constant=False)\n",
    "    \n",
    "    print(\"\\n=== Power Law with Additive Constant ===\")\n",
    "    results_additive = analyze_power_law_regression(data, [parameters], include_additive_constant=True)\n",
    "    \n",
    "    return results_standard, results_additive\n",
    "\n",
    "# For your existing workflow, you can now use:\n",
    "# results = analyze_power_law_regression(data, [['k', 'o']], include_additive_constant=True)\n",
    "\n",
    "\n",
    "def read_data(fn: str, function_name: str, parameter_names: dict[str, int]) -> dict[str, npt.NDArray[np.float64]]:\n",
    "    import json\n",
    "    with open(fn, 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "    assert function_name in json_data.keys()\n",
    "    samples = json_data[function_name]\n",
    "    # sample1 = samples[0]\n",
    "    # assert len(sample1) == len(parameter_names) + 1\n",
    "    samples_arr = np.array(samples, dtype=np.float64)\n",
    "    \n",
    "    data = {}\n",
    "    for parameter_name, i in parameter_names.items():\n",
    "        data[parameter_name] = samples_arr[:,i]\n",
    "    data['performance'] = samples_arr[:,-1]\n",
    "    \n",
    "    from pprint import pprint\n",
    "    # print('data:')\n",
    "    # pprint(data)\n",
    "    return data\n",
    "\n",
    "\n",
    "# alg_names = ['var_direct_np', 'var_hypo']\n",
    "\n",
    "# print('*'*80)\n",
    "# print(f'* fit k: performance of var_direct_np')\n",
    "# data = read_data('running_time_data_compute_var.json', 'var_direct_np', {'k': 1})\n",
    "# # results_full = analyze_power_law_regression(data, [['k']], include_additive_constant=False)\n",
    "# results_full = analyze_power_law_regression(data, [['k']], include_additive_constant=True)\n",
    "\n",
    "# print('\\n' + '*'*80)\n",
    "# print(f'* fit o: performance of var_hypo')\n",
    "# data = read_data('running_time_data_compute_var.json', 'var_hypo', {'o': 2})\n",
    "# # results_full = analyze_power_law_regression(data, [['o']], include_additive_constant=False)\n",
    "# results_full = analyze_power_law_regression(data, [['o']], include_additive_constant=True)\n",
    "\n",
    "# for alg_name in alg_names:\n",
    "#     print('\\n' + '*'*80)\n",
    "#     print(f'* fit k, o: performance of {alg_name}')\n",
    "#     data = read_data('running_time_data_compute_var.json', alg_name, {'k': 1, 'o': 2})\n",
    "#     # results_full = analyze_power_law_regression(data, [['k', 'o']], include_additive_constant=False)\n",
    "#     results_full = analyze_power_law_regression(data, [['k', 'o']], include_additive_constant=True)\n",
    "\n",
    "# for alg_name in alg_names:\n",
    "#     print('\\n' + '*'*80)\n",
    "#     print(f'* fit n, k, o, g: performance of {alg_name}')\n",
    "#     data = read_data('running_time_data_compute_var.json', alg_name, {'n': 0, 'k': 1, 'o': 2, 'g': 3})\n",
    "#     # results_full = analyze_power_law_regression(data, [['n', 'k', 'o', 'g']], include_additive_constant=False)\n",
    "#     results_full = analyze_power_law_regression(data, [['n', 'k', 'o', 'g']], include_additive_constant=True)\n",
    "\n",
    "alg_names = ['mean_direct_np', 'mean_hypo']\n",
    "\n",
    "print('*'*80)\n",
    "print(f'* fit k: performance of mean_direct_np')\n",
    "data = read_data('running_time_data_compute_mean.json', 'mean_direct_np', {'k': 1})\n",
    "# results_full = analyze_power_law_regression(data, [['k']], include_additive_constant=False)\n",
    "results_full = analyze_power_law_regression(data, [['k']], include_additive_constant=True)\n",
    "\n",
    "print('\\n' + '*'*80)\n",
    "print(f'* fit o: performance of mean_hypo')\n",
    "data = read_data('running_time_data_compute_mean.json', 'mean_hypo', {'o': 2})\n",
    "# results_full = analyze_power_law_regression(data, [['o']], include_additive_constant=False)\n",
    "results_full = analyze_power_law_regression(data, [['o']], include_additive_constant=True)\n",
    "\n",
    "for alg_name in alg_names:\n",
    "    print('\\n' + '*'*80)\n",
    "    print(f'* fit k, o: performance of {alg_name}')\n",
    "    data = read_data('running_time_data_compute_mean.json', alg_name, {'k': 1, 'o': 2})\n",
    "    # results_full = analyze_power_law_regression(data, [['k', 'o']], include_additive_constant=False)\n",
    "    results_full = analyze_power_law_regression(data, [['k', 'o']], include_additive_constant=True)\n",
    "\n",
    "for alg_name in alg_names[1:]:\n",
    "    print('\\n' + '*'*80)\n",
    "    print(f'* fit n, k, o: performance of {alg_name}')\n",
    "    data = read_data('running_time_data_compute_mean.json', alg_name, {'n': 0, 'k': 1, 'o': 2})\n",
    "    # results_full = analyze_power_law_regression(data, [['n', 'k', 'o', 'g']], include_additive_constant=False)\n",
    "    results_full = analyze_power_law_regression(data, [['n', 'k', 'o']], include_additive_constant=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "a26a117c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      predicted_mean_hypo(k,o)='212 μs'\n",
      "     actual_mean_hypo(n,k,o,g)='258 μs'\n",
      "\n",
      "   predicted_mean_direct_np(k)='66.1 μs'\n",
      "actual_mean_direct_np(n,k,o,g)='359 μs'\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import gamma\n",
    "import math\n",
    "import importlib\n",
    "import gamma\n",
    "importlib.reload(gamma)\n",
    "\n",
    "def predicted_mean_direct_np(k: float) -> str:\n",
    "    # return gamma.format_time(0.0000001010 * k**0.952)\n",
    "    return gamma.format_time(0.0000000927 * k**0.951 * o**0.103)\n",
    "\n",
    "def predicted_mean_hypo(k: float, o: float) -> str:\n",
    "    return gamma.format_time(0.0002378857 * k**-0.017 * o**0.849)\n",
    "    # return gamma.format_time(0.0001912085 * o**0.873)\n",
    "    # return gamma.format_time(0.0000032289 * n**0.051 * k**0.274 * o**0.904)\n",
    "\n",
    "def actual_mean_direct_np(n, k, o, g) -> str:\n",
    "    start = time.perf_counter()\n",
    "    gamma.mean_direct_np(n, k, o, g)\n",
    "    end = time.perf_counter()\n",
    "    return gamma.format_time(end - start)\n",
    "\n",
    "def actual_mean_hypo(n, k, o, g) -> str:\n",
    "    start = time.perf_counter()\n",
    "    gamma.mean_hypo(n, k, o, g)\n",
    "    end = time.perf_counter()\n",
    "    return gamma.format_time(end - start)\n",
    "\n",
    "n = 10**6\n",
    "k = round(math.sqrt(n))\n",
    "o = 1\n",
    "g = 1\n",
    "\n",
    "print(f'      {predicted_mean_hypo(k,o)=}')\n",
    "print(f'     {actual_mean_hypo(n,k,o,g)=}')\n",
    "print()\n",
    "print(f'   {predicted_mean_direct_np(k)=}')\n",
    "print(f'{actual_mean_direct_np(n,k,o,g)=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "id": "58fa2998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   predicted_hypo(k,o)='6.83 ms'\n",
      "  actual_hypo(n,k,o,g)='3.77 ms'\n",
      "\n",
      "   predicted_direct(k)='46.5 ms'\n",
      "actual_direct(n,k,o,g)='19.1 ms'\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import gamma\n",
    "import math\n",
    "import importlib\n",
    "import gamma\n",
    "importlib.reload(gamma)\n",
    "\n",
    "def predicted_direct(k: float) -> str:\n",
    "    return gamma.format_time(0.0000000666 * k**0.974)\n",
    "\n",
    "def predicted_hypo(k: float, o: float) -> str:\n",
    "    # return gamma.format_time(0.0002419766 * o**2.341 + 0.000374)\n",
    "    # return gamma.format_time(0.0007250757 * k**-0.065 * o**1.928)\n",
    "    return gamma.format_time(0.0007118038 * k**-0.064 * o**1.954)\n",
    "    # return gamma.format_time(0.0003820319 * o**1.958)\n",
    "    # return gamma.format_time(0.0003939771 * o**1.954)\n",
    "\n",
    "def actual_direct(n, k, o, g) -> str:\n",
    "    start = time.perf_counter()\n",
    "    gamma.var_direct_np(n, k, o, g)\n",
    "    end = time.perf_counter()\n",
    "    return gamma.format_time(end - start)\n",
    "\n",
    "def actual_hypo(n, k, o, g) -> str:\n",
    "    start = time.perf_counter()\n",
    "    gamma.var_hypo(n, k, o, g)\n",
    "    end = time.perf_counter()\n",
    "    return gamma.format_time(end - start)\n",
    "\n",
    "n = 10**12\n",
    "k = round(math.sqrt(n))\n",
    "o = 5\n",
    "g = 1\n",
    "\n",
    "print(f'   {predicted_hypo(k,o)=}')\n",
    "print(f'  {actual_hypo(n,k,o,g)=}')\n",
    "print()\n",
    "print(f'   {predicted_direct(k)=}')\n",
    "print(f'{actual_direct(n,k,o,g)=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "9aedb2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.5 μs ± 594 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n",
      "2.86 μs ± 44 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "from scipy.special import comb as sp_comb\n",
    "from scipy.special import binom\n",
    "import math\n",
    "\n",
    "def reciprocals(n: int, k: int, o: int, g: int, method: str) -> npt.NDArray[np.float64]:\n",
    "    top_values = np.arange(k) * g + n\n",
    "    if method == 'binom':\n",
    "        binomial_values = binom(top_values, o)\n",
    "    elif method == 'sp_comb_exact':\n",
    "        binomial_values = np.array([sp_comb(top_val, o, exact=True) for top_val in top_values])\n",
    "    elif method == 'sp_comb':\n",
    "        binomial_values = sp_comb(top_values, o, exact=False)\n",
    "    elif method == 'math_comb':\n",
    "        binomial_values = np.array([math.comb(top_val, o) for top_val in top_values])\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return 1.0 / binomial_values\n",
    "\n",
    "n = 10**12\n",
    "k = round(math.sqrt(n))\n",
    "o = 5\n",
    "g = 3\n",
    "# print(f'{reciprocals(n, k, o, g, 'math_comb')=}')\n",
    "# print(f'{reciprocals(n, k, o, g, 'binom')=}')\n",
    "# print(f'{reciprocals(n, k, o, g, 'sp_comb')=}')\n",
    "# print(f'{reciprocals(n, k, o, g, 'sp_comb_exact')=}')\n",
    "# for method in ['math_comb', 'sp_comb', 'binom', 'sp_comb_exact']:\n",
    "#     print(f'{method}: ', end = '')\n",
    "#     %timeit reciprocals(n, k, o, g, method)\n",
    "# sp_comb(np.array([5,10]), np.array([2,2]), exact=True)\n",
    "%timeit sp_comb(10**3, 10**2, exact=True)\n",
    "%timeit math.comb(10**3, 10**2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
